{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1e05e0",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "Activation functions helps to determine the output of a neural network.These type of functions are attached to each neuron in\n",
    "the network,and determines whether it should be activated or not, based on whether each neuron's input is relevant for the model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec36698e",
   "metadata": {},
   "source": [
    "Activation function also helps to normalize the output\n",
    "of each neuron to range between 1 and 0 or between -1 to 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3752141a",
   "metadata": {},
   "source": [
    "![](1_BMSfafFNEpqGFCNU4smPkg (1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7331b5f",
   "metadata": {},
   "source": [
    "<img src=\"activation photo.png\"/>\n",
    "\n",
    "* In a neural network,inputs are fed into the neurons in the input layer. Each neuron has a weight and multiplying the input number with the weight gives the output of the neuron, which is transfered to the next layer.\n",
    "\n",
    "* The activation function is a mathematical \"gate\" in between the input feeding the current neuron and its output going to the next layer. It can be as simple as step function that turns the neuron output on and off, depending on a rule or threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad7ff4",
   "metadata": {},
   "source": [
    "Neural networks use non-linear activation functions which can help the \n",
    "network learn complex data, compute and learn almost any function representing a question, and provide accurate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a418ca9",
   "metadata": {},
   "source": [
    "## Commonly used activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695a8db",
   "metadata": {},
   "source": [
    "### 1.Sigmoid function = `1/1+e**-x`\n",
    "\n",
    "The graph of sigmoid and it's derivate are :\n",
    "             \n",
    "<img src=\"sigmoid.png\"/>\n",
    "\n",
    "The sigmoid function is the most frequently used activation function in the begining of deep learning. It is a smoothing funtion that is easy to derive.\n",
    "\n",
    "* sigmoid function always gives to output between (0,1). We can think of propability,but in the strict sense,don't treat it as probability. The sigmoid function was once more popular. It can be thought of as the firing rate of a neuron. IN the middle where the slocpe is relatively large. It is the sensitive area of the neuron. On the sides where the slope is very gentle, it is the  neuron's inhibitory area.\n",
    "\n",
    "* The function it self as certian defects:\n",
    "\n",
    "* 1)When the input is slightly away form the coordinat origin,the gradient of the function becomes very small, almost zero.In the process of neural network backpropagation,we all use the chain rule of differntial to calculate the differential of each weight w. When the backpropagation passes through the sigmoid function, the differential on this chain is very small. Moreover, it may pass through many sigmoid functions, Which will eventually cause the weight w to have little effect on the loss function, which is not conducive to the optimization of the weight. This is the problem called \"gardient saturation or gradient dispersion\"\n",
    "\n",
    "* 2)The function output is not centered on 0, which will reduce the efficiency of weight update.\n",
    "\n",
    "* 3)The sigmoid function performs exponential operations , which is slower for computers.\n",
    "\n",
    "#### Advantages of sigmoid Function:-\n",
    "* 1.Smooth gradient,preventing\"jumps\" in output values.\n",
    "* 2.Output values bound between 0 and 1, normalizing the output of each neuron.\n",
    "* 3. Clear predictions, i.e very close to 1 or 0\n",
    "\n",
    "#### Major disadvantages:\n",
    "* 1.prone to gradient vanishing\n",
    "* 2.Function output is not zero-centered\n",
    "* 3.Power operations are relatively time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc49a18",
   "metadata": {},
   "source": [
    "### 2.Threshold function (tanh function)\n",
    "\n",
    "The tanh function formula and curve are as follows:\n",
    "\n",
    "<img src=\"tanhfunction.png\"/>\n",
    "<img src=\"tanhgraphs.png\"/>\n",
    "\n",
    "* Tanh is a hyperbolic tanget function. The curves of tanh function and sigmoid function are relatively similar. Let's compare them.First of all, when the input is large or small,the output is almost smooth and the gradient is small, which is not conductive to weight update. The difference is the output interval.\n",
    "\n",
    "* The output inteval of tanh is (-1,1), and the whole function is 0-centric,which is better than sigmoid.\n",
    "\n",
    "* In general binay classification problems, the tanh function is used for the hidden layer and the sigmod function is used for the output layer. However these are not static, and teh specific activation function to be used must be analyzed according to the specific problem. or a depends on debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f7d5f",
   "metadata": {},
   "source": [
    "### 3.Rectified Linear Unit (`ReLU`) function\n",
    "\n",
    "ReLU function formula and curve are as follows\n",
    "\n",
    "`ReLU = max(0,x)`\n",
    "<img src=\"relugraphs.png\"/>\n",
    "\n",
    "The ReLU function is actually a function that takes the maximum value.Note that this is not fully interval-derivable,but we can take sub-gradient as shown in the figure above .Althought RELU is simple.It is an important achievement in recent years.\n",
    "\n",
    "The ReLU(`Rectified Linear Unit`)funticon is an activatio function that is currently more popular.Compared with the sigmoid function and teh tanh function, it has the following advantages:\n",
    "\n",
    "#### Advantages:\n",
    "\n",
    "* 1.When the input is positive, there is no gradient saturation problem.\n",
    "\n",
    "* 2.the calculation speed is much faster. The ReLU function has only a linear function has only relationship. Whether it is forward or backward, it is much faster thatn sigmoid and tanh.(sigmoid and tanh need to calculate the exponent,which will be slower)\n",
    "\n",
    "#### Disadvantages:\n",
    "\n",
    "* 1.When the input is negative,ReLU is completely inactive, which means that once a negative number is entered, ReLU will die. In this way,in the forward propagation process. It is not a problem. In some areas it is sensitive and not in some areas. But in the backpropagation process, if you enter a negative number, the gradient will be completely zero, which has the same problem as the sigmoid function and tanh function.\n",
    "\n",
    "* 2.We find that the output of the ReLU function is either 0 or a positive number, which means that the ReLU function is not a `0-centric function`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47affe15",
   "metadata": {},
   "source": [
    "### 4.Leaky ReLU function:\n",
    "\n",
    "`f(x) = max(0.01z,z)`\n",
    "<img src=\"leakyrelugraphs.png\"/>\n",
    "\n",
    "* In order to solve the Dead ReLU rpblem, people proposed to set teh first half of ReLU 0.01x insted of 0. Another intutive idea is a parameter-based method, parametric ReLU:`f(x)=max(alpha*z,z)`,which alpha can be learned from back propagation. In theory,Leaky RelU has all the advantages of ReLU,plus there will be no problems with Dead ReLU,but in actual operation , it has not been fully proved taht leaky ReLU is always better than ReLU.\n",
    "\n",
    "#### Disadvantages:\n",
    "* 1. when we get negative values of z then weight old is approx equal to weight new in back propagation this leads to vanishing gradient desent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9e0e89",
   "metadata": {},
   "source": [
    "### 5.ELU (Exponential Linear Units) function:\n",
    "\n",
    "<img src=\"elu.png\"/>\n",
    "\n",
    "<img src=\"elugraphs.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16422f3b",
   "metadata": {},
   "source": [
    "ELU is also proposed to solve the problems of ReLU.Obviously,ELU has all the advantages of ReLU\n",
    "\n",
    "#### Advantages:\n",
    "* 1.No Dead ReLU issues\n",
    "* 2.The mean of the output is close to 0.`zero-centered`\n",
    "\n",
    "#### Disadvantages:\n",
    "* One small problem is that it is slightly more computationally intensive. Similar to Leaky ReLU.Although theoretically better than ReLU, there is currently no good evidence in practice that ELU is always better than ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7bfa7",
   "metadata": {},
   "source": [
    "### 6.PReLU (Parametric ReLU):\n",
    "\n",
    "\n",
    "The graph of prelu looks like this:\n",
    "<img src=\"prelu.png\"/>\n",
    "\n",
    "##### ReLU  vs. PReLU . For PReLU, the coeffcient of the negative part is not constant and is adaptively learned.\n",
    "\n",
    "PReLU is also an improved version of ReLU. In the neaative region,PReLU has a small slope, which can also avoid teh problem of ReLU death. Compared to ELU, PreLU is a linear operation in the negative region . Although the slpe is small,it does no tnd to 0, which is a certain advantage.\n",
    "\n",
    "\n",
    "* If we look at the graph of PReLU. The parameter `a` is generally a  number betwwn 0 and 1, and it is generally relatively small, such as a few zeros. When a=0.01, we call PReLU as Leaky ReLU, it is regarded as a special case of PReLU.\n",
    "\n",
    "Above yi is any input on the ith channel an ai is the negative slpe which is a `learnable parameter`.\n",
    "\n",
    "* if ai=0, f becomes ReLU\n",
    "* if ai>0, f becomes leaky ReLU\n",
    "* if ai is a learnable parameter, f becomes PReLU\n",
    "\n",
    "<img src=\"relugraphs.png\"/>\n",
    "<img src=\"leakyrelugraphs.png\"/>\n",
    "\n",
    "<img src=\"reluvarients.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce31ffd",
   "metadata": {},
   "source": [
    "### 7.Swish (A Self-Gated) Function: \n",
    "`f(x) = x*sigmoid(x)`\n",
    "\n",
    "The graph os swish looks like this:\n",
    "\n",
    "<img src=\"swishgraph.png\"/>\n",
    "\n",
    "* Swish design was inspired by the use of sigmoid functions for gating in LSTMs and highway networks. We use the same value for gating to simplify the gating mechanishm, which is called `self-gating`.\n",
    "\n",
    "#### Advantages:\n",
    "\n",
    "* It only requires a simple scalar input, while normal gating requires multiple scalar inputs.\n",
    "* This feature enables self-gated activato functions such as Swish to easily replace activation functions that take a single scalar as input (such as ReLU) without changing the hidden capacity or number of parameters.\n",
    "\n",
    "* 1.Unboundness is helpful to prevent gradient from gradually approaching 0 during slow training, causing saturation.At the same time, being bounded has advantages,because boundded active functions can have strong regualrization,and larger negative inputs will be resolved.\n",
    "\n",
    "* 2.At the same time, smoothness also plays an important role in optimization and generalization.\n",
    "\n",
    "#### Remember:\n",
    "when ever we have a deep and large neural network (more than 40 layers) then only we should use Swish function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982d62d",
   "metadata": {},
   "source": [
    "### 8.SoftPlus Function:\n",
    "\n",
    "`f(x) = ln(1+exp(x))`\n",
    "\n",
    "we can see the difference between the softplus and ReLU:\n",
    "<img src=\"softplus.png\"/>\n",
    "The graphs of softplus and its derivate:\n",
    "<img src=\"softplusgraph.png1.png\"/>\n",
    "\n",
    "The softplus is similar to the ReLU function,but it is relatively smooth . It is unilateral suppression like ReLU. It has a wide acceptance range(0,inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085db23",
   "metadata": {},
   "source": [
    " ### 9.SoftMax Function:\n",
    " \n",
    " <img src=\"softmax.png\"/>\n",
    " \n",
    "* For an arbitary real vector of length K, Softmax can compress it into a ral bector of lenth K with a value in the range(0,1) and the sum of the elements in the vector is 1.\n",
    " \n",
    "* It also has many applications in Multiclass Classification and neural networks.Softmax is different from the normal max function:the max function only outputs the largest value, and Softmax ensures that smaller values have a smaller probability and will not be discarded directly it is \"max\" that is \"soft\".\n",
    "\n",
    "* The denominator of the Softmax function combines all factors of the orignal output value, which means that the different probabilites obtained by the softmax are related to each other. In the case of binary classification, for sigmoid there are:\n",
    "\n",
    " <img src=\"softmaxgraph.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5248df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
